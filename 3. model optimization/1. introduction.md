# the model optimizer
We were using pretrained models till now in the course. If a model is not one of the pre-converted models that OpenVINO™ provides, we have to optimize the model; it is a required step to move onto the Inference Engine.

## job of model optimizer
* converts the model from multiple frameworks to an intermediate representation (IR) for the inference engine.
* performs optimization of routines and structures to shrink the execution time and speed up the inference.

#### note
* it will not improve the inference accuracy.
* it may lower the precision levels.

## configure the optimizer

given that you already have OpenVINO™ installed. You can navigate to your OpenVINO™ install directory first, which is usually `/opt/intel/openvino`. Then, head to `/deployment_tools/model_optimizer/install_prerequisites`, and run the `install_prerequisites.sh` script therein.