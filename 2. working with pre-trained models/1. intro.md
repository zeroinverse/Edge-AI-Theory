# working with pre-trained models
We will be using The Intel® Distribution of OpenVINO™ Toolkit throughout this series.

#### The Intel® Distribution of OpenVINO™ Toolkit
It is developed by Intel®, and helps support fast inference across Intel® CPUs, GPUs, FPGAs and Neural Compute Stick with a common API. OpenVINO™ can take models built with multiple different frameworks, like TensorFlow or Caffe, and use its Model Optimizer to optimize for inference. This optimized model can then be used with the Inference Engine, which helps speed inference on the related hardware. It also has a wide variety of Pre-Trained Models already put through Model Optimizer.  
By optimizing for model speed and size, OpenVINO™ enables running at the edge. This does not mean an increase in inference accuracy - this needs to be done in training beforehand.

#### reason for going OpenVINO™
The smaller, quicker models OpenVINO™ generates, along with the hardware optimizations it provides, are great for lower resource applications. For example, an IoT device does not have the benefit of multiple GPUs and unlimited memory space to run its apps.

#### pre-trained models
pre-trained models refer to models where training has already occurred, and often have high, or even cutting-edge accuracy. Using pre-trained models avoids the need for large-scale data collection and long, costly training.  
We can plug these directly into our apps and start exploring.

##### headstart guy? follow the OpenVino documentation [here](https://software.intel.com/en-us/openvino-toolkit/documentation/pretrained-models)

#### Let's move foreward, shall we?

Cheers!